{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1-1: Preparing data for analysis with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load elders dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = local[*], app id = local-1558352204896)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5137dcd5\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"src/scala/register_tables.scala\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prescription dataset===\n",
      "registering patients\n",
      "registering prescriptions\n",
      "registering drugs\n",
      "ok!\n",
      "===Hospitalization dataset===\n",
      "registering all\n",
      "registering patients\n",
      "registering prescriptions\n",
      "registering drugs\n",
      "ok!\n",
      "===NPR Elders===\n",
      "ok!\n",
      "+--------+--------------------------+-----------+\n",
      "|database|tableName                 |isTemporary|\n",
      "+--------+--------------------------+-----------+\n",
      "|        |elders                    |true       |\n",
      "|        |elders_drugs              |true       |\n",
      "|        |elders_patients           |true       |\n",
      "|        |elders_prescriptions      |true       |\n",
      "|        |npr_elders                |true       |\n",
      "|        |prescription_drugs        |true       |\n",
      "|        |prescription_patients     |true       |\n",
      "|        |prescription_prescriptions|true       |\n",
      "+--------+--------------------------+-----------+\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "register_tables(spark, \"spark-warehouse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elders: org.apache.spark.sql.DataFrame = [id: string, birthyear: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val elders = spark.sql(\"select * from elders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- birthyear: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- fylke_id: integer (nullable = true)\n",
      " |-- fylke_name: integer (nullable = true)\n",
      " |-- drugcode: string (nullable = true)\n",
      " |-- DDD_value: string (nullable = true)\n",
      " |-- DDD_unit: string (nullable = true)\n",
      " |-- VareNavn: string (nullable = true)\n",
      " |-- prescription_year: timestamp (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- Diff_UtleveringDato: integer (nullable = true)\n",
      " |-- prescriber_id: string (nullable = true)\n",
      " |-- prescriber_birthyear: integer (nullable = true)\n",
      " |-- prescriber_gender: integer (nullable = true)\n",
      " |-- prescriber_no_id: integer (nullable = true)\n",
      " |-- Hjemmel: string (nullable = true)\n",
      " |-- Hjemmelnr: integer (nullable = true)\n",
      " |-- Kategori: string (nullable = true)\n",
      " |-- KategoriNr: integer (nullable = true)\n",
      " |-- OrdinasjonAntallPakninger: float (nullable = true)\n",
      " |-- OrdinasjonAntallDDD: float (nullable = true)\n",
      " |-- death_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elders.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospitalized: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, birthyear: int ... 21 more fields]\n",
       "unhospitalized: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, birthyear: int ... 21 more fields]\n",
       "unhospitalized_alive: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, birthyear: int ... 21 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hospitalized = elders.where(\"diff_utleveringdato is not null\")\n",
    "val unhospitalized = elders.where(\"diff_utleveringdato is null\")\n",
    "val unhospitalized_alive = unhospitalized.where(\"death_timestamp is null\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescriptions of dead unhospitalized patients = ~9M\n",
      "8972091\n",
      "prescriptions of live, unhospitalized patients = ~30M\n",
      "30642882\n"
     ]
    }
   ],
   "source": [
    "println(\"prescriptions of dead unhospitalized patients = ~9M\")\n",
    "println(unhospitalized.count-unhospitalized_alive.count)\n",
    "println(\"prescriptions of live, unhospitalized patients = ~30M\")\n",
    "println(unhospitalized_alive.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now we are concerned with two populations, unhospitalized_alive, and hospitalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's assume that we only want prescriptions and dates of unhospitalized patients\n",
    "* timestamp type is encoded as epoch time, thus casting it to integer will yield an epoch int\n",
    "* We take the following variables\n",
    "    * id\n",
    "    * birthyear\n",
    "    * drugcode\n",
    "    * timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uh_a_drug_date: org.apache.spark.sql.DataFrame = [id: string, birthyear: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uh_a_drug_date = unhospitalized_alive\n",
    "    .select( $\"id\", $\"birthyear\", $\"drugcode\", $\"timestamp\".cast(\"integer\") )\n",
    "    //.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take 100.000 patients from the unhospitalized, \"HEALTHY\" dataset\n",
    "#### With their prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_patients: Int = 100000\n",
       "unhospitalized_10k_sample: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string]\n",
       "unhosp_sample_prescriptions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [pasientlopenr: string, id: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n_patients = 100000\n",
    "val unhospitalized_10k_sample = uh_a_drug_date.select($\"id\").distinct.limit(n_patients).cache\n",
    "val unhosp_sample_prescriptions = unhospitalized_10k_sample\n",
    "    .select($\"id\".as(\"pasientlopenr\"))\n",
    "    .join(elders)\n",
    "    .where(\"id=pasientlopenr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospitalized patients have a different timestamp encoding\n",
    "* We take the following\n",
    "    * Id\n",
    "    * birthyear\n",
    "    * drugcode\n",
    "    * timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val h_drug_reldate = hospitalized\n",
    "//    .select($\"id\",$\"birthyear\",$\"drugcode\",$\"diff_utleveringdato\")\n",
    "//    .where(\"diff_utleveringdato < 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hospitalizations = spark.sql(\"select * from npr_elders\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take an equal number of hospitalized elders\n",
    "* take patients with no more than 5 hospitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eligible_hospitalized: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [pasientlopenr: string, num_hospitalizations: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eligible_hospitalized = hospitalizations\n",
    "    .select(\"pasientlopenr\", \"num_hospitalizations\",\"n_prescriptions\", \"hovedtilstand\")\n",
    "    .filter(\"num_hospitalizations <= 10\")\n",
    "    .distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hospitalized_10k_sample = hospitalized\n",
    "    .join(eligible_hospitalized)\n",
    "    .where(\"pasientlopenr=id\")\n",
    "    .select(\"id\")\n",
    "    .distinct\n",
    "    .limit(n_patients)\n",
    "    .cache\n",
    "//hospitalized_10k_sample.show(10)\n",
    "hospitalized_10k_sample.distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take all prescriptions of the selected hospitalized patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prescriptions of hospitalized patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hosp_sample_prescriptions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [pasientlopenr: string, id: string ... 22 more fields]\n",
       "res8: Long = 11099081\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hosp_sample_prescriptions = hospitalized_10k_sample.select($\"id\".as(\"pasientlopenr\"))\n",
    "    .join(elders)\n",
    "    .where(\"id=pasientlopenr\")\n",
    " hosp_sample_prescriptions.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospitalization information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hosp_sample_nprdata = hospitalized_10k_sample\n",
    "    .join(hospitalizations)\n",
    "    .where(\"id=pasientlopenr\")\n",
    "    .select(\"diffdager_inn\",\"diffdager_ut\", \"id\")\n",
    "\n",
    "//hosp_sample_nprdata.show(2)\n",
    "hosp_sample_nprdata.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "* N assumed \"Healthy\" people\n",
    "* Hospitalized:\n",
    "    * N hospitalized people with less than 10 hospitalizations in the period\n",
    "    * All of these patients hospitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputfolder: String = spark-warehouse/experiment1-data/\n",
       "hospdir: String = hospitalized/\n",
       "healthydir: String = healthy/\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputfolder = \"spark-warehouse/experiment1-data/\"\n",
    "val hospdir = \"hospitalized/\"\n",
    "val healthydir = \"healthy/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hospitalization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "hosp_sample_nprdata\n",
    "    .sort(asc(\"id\"))\n",
    "    .write\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .parquet(outputfolder+hospdir+\"hospitalizations/\")\n",
    "println(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hospitalized patients' prescription data\n",
    "* Take only 4 levels of the ATC hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11099081\n"
     ]
    }
   ],
   "source": [
    "println(hosp_sample_prescriptions.count)\n",
    "//hosp_sample_prescriptions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"src/scala/transformations.scala\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "hosp_sample_prescriptions\n",
    "    .select($\"id\"\n",
    "            ,$\"diff_utleveringdato\"\n",
    "            ,takeNatcLevels(4)($\"drugcode\").as(\"4L_atccode\")\n",
    "           )\n",
    "    .sort(asc(\"id\"))\n",
    "    .write\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .parquet(outputfolder+hospdir+\"4L_prescriptions\")\n",
    "//    .show(5)\n",
    "println(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell verifies that the intersection of IDs between NPR and hosp_prescription is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "common_ids: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, id2: string]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val common_ids = hosp_sample_nprdata.select(\"id\").distinct\n",
    "    .join(hosp_sample_prescriptions.select($\"id\".as(\"id2\")).distinct)\n",
    "    .where(\"id = id2\")\n",
    "println(common_ids.count)\n",
    "//common_ids.show(5)\n",
    "println(hosp_sample_nprdata.select(\"id\").distinct.count)\n",
    "println(hosp_sample_prescriptions.select(\"id\").distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save \"healthy\" patients prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5853822\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "println(unhosp_sample_prescriptions.count)\n",
    "unhosp_sample_prescriptions\n",
    "    .select($\"id\"\n",
    "            ,$\"timestamp\"\n",
    "            ,takeNatcLevels(4)($\"drugcode\").as(\"4L_atccode\")\n",
    "           )\n",
    "    .write\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .parquet(outputfolder+healthydir+\"4L_prescriptions\")\n",
    " //   .show(5)\n",
    "println(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
